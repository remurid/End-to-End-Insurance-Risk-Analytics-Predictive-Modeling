{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9db4880",
   "metadata": {},
   "source": [
    "# Insurance Risk Hypothesis Testing\n",
    "\n",
    "This notebook performs statistical validation of key risk driver hypotheses for insurance segmentation, using modular code. The workflow includes:\n",
    "- Data loading and preprocessing\n",
    "- Metric calculation (claim frequency, severity, margin)\n",
    "- Data segmentation for A/B testing\n",
    "- Statistical testing\n",
    "- Analysis and business recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21c19b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory (one level up from notebooks/) to sys.path\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "import pandas as pd\n",
    "from utils.data_prep import load_data, preprocess_data\n",
    "from utils.metrics import compute_claim_frequency, compute_claim_severity, compute_margin\n",
    "from utils.segmentation import split_groups\n",
    "from utils.stat_tests import run_chi2_test, run_ttest\n",
    "from utils.reporting import interpret_result, business_recommendation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db0da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Personal Projects\\TENx\\Week3\\code\\End-to-End Insurance Risk Analytics & Predictive Modeling\\src\\utils\\data_prep.py:8: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(filepath, delimiter='|')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UnderwrittenCoverID', 'PolicyID', 'TransactionMonth', 'IsVATRegistered', 'Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType', 'MaritalStatus', 'Gender', 'Country', 'Province', 'PostalCode', 'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'mmcode', 'VehicleType', 'RegistrationYear', 'make', 'Model', 'Cylinders', 'cubiccapacity', 'kilowatts', 'bodytype', 'NumberOfDoors', 'VehicleIntroDate', 'CustomValueEstimate', 'AlarmImmobiliser', 'TrackingDevice', 'CapitalOutstanding', 'NewVehicle', 'WrittenOff', 'Rebuilt', 'Converted', 'CrossBorder', 'NumberOfVehiclesInFleet', 'SumInsured', 'TermFrequency', 'CalculatedPremiumPerTerm', 'ExcessSelected', 'CoverCategory', 'CoverType', 'CoverGroup', 'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType', 'TotalPremium', 'TotalClaims']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UnderwrittenCoverID</th>\n",
       "      <th>PolicyID</th>\n",
       "      <th>TransactionMonth</th>\n",
       "      <th>IsVATRegistered</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>LegalType</th>\n",
       "      <th>Title</th>\n",
       "      <th>Language</th>\n",
       "      <th>Bank</th>\n",
       "      <th>AccountType</th>\n",
       "      <th>...</th>\n",
       "      <th>ExcessSelected</th>\n",
       "      <th>CoverCategory</th>\n",
       "      <th>CoverType</th>\n",
       "      <th>CoverGroup</th>\n",
       "      <th>Section</th>\n",
       "      <th>Product</th>\n",
       "      <th>StatutoryClass</th>\n",
       "      <th>StatutoryRiskType</th>\n",
       "      <th>TotalPremium</th>\n",
       "      <th>TotalClaims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [UnderwrittenCoverID, PolicyID, TransactionMonth, IsVATRegistered, Citizenship, LegalType, Title, Language, Bank, AccountType, MaritalStatus, Gender, Country, Province, PostalCode, MainCrestaZone, SubCrestaZone, ItemType, mmcode, VehicleType, RegistrationYear, make, Model, Cylinders, cubiccapacity, kilowatts, bodytype, NumberOfDoors, VehicleIntroDate, CustomValueEstimate, AlarmImmobiliser, TrackingDevice, CapitalOutstanding, NewVehicle, WrittenOff, Rebuilt, Converted, CrossBorder, NumberOfVehiclesInFleet, SumInsured, TermFrequency, CalculatedPremiumPerTerm, ExcessSelected, CoverCategory, CoverType, CoverGroup, Section, Product, StatutoryClass, StatutoryRiskType, TotalPremium, TotalClaims]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 52 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "DATA_PATH = os.path.abspath(os.path.join('..', 'data', 'raw', 'MachineLearningRating_v3.txt'))\n",
    "df_raw = load_data(DATA_PATH)\n",
    "print('Rows after loading:', len(df_raw))\n",
    "print('Rows with missing values:', df_raw.isnull().any(axis=1).sum())\n",
    "# Only drop rows with missing values in required columns\n",
    "required_cols = ['TotalClaims', 'TotalPremium', 'Province', 'Gender', 'PostalCode']\n",
    "df = df_raw.drop_duplicates()\n",
    "df = df.dropna(subset=required_cols)\n",
    "print('Rows after preprocessing:', len(df))\n",
    "print(df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1a03d",
   "metadata": {},
   "source": [
    "## Hypothesis 1: Risk Differences Across Provinces\n",
    "\n",
    "- **Null Hypothesis (H₀):** There are no risk differences across provinces.\n",
    "- **Metrics:** Claim Frequency, Claim Severity, Margin\n",
    "- **Test:** Chi-squared (frequency), t-test (severity, margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f3e2b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Frequency: Fail to reject the null hypothesis (p = nan).\n",
      "Claim Severity: Fail to reject the null hypothesis (p = nan).\n",
      "Margin: Fail to reject the null hypothesis (p = nan).\n",
      "No significant difference found for provinces between Gauteng and Western Cape. No change recommended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Personal Projects\\TENx\\Week3\\code\\End-to-End Insurance Risk Analytics & Predictive Modeling\\src\\utils\\metrics.py:13: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(group_col).apply(lambda x: x['TotalPremium'].sum() - x['TotalClaims'].sum())\n",
      "d:\\Personal Projects\\TENx\\Week3\\code\\End-to-End Insurance Risk Analytics & Predictive Modeling\\.venv\\Lib\\site-packages\\scipy\\stats\\contingency.py:135: RuntimeWarning: invalid value encountered in divide\n",
      "  expected = reduce(np.multiply, margsums) / observed.sum() ** (d - 1)\n",
      "d:\\Personal Projects\\TENx\\Week3\\code\\End-to-End Insurance Risk Analytics & Predictive Modeling\\.venv\\Lib\\site-packages\\scipy\\_lib\\deprecation.py:234: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "# Compute metrics for provinces\n",
    "df['TotalClaims'] = df['TotalClaims'].astype(int)\n",
    "province_freq = df.groupby('Province')['TotalClaims'].apply(lambda x: (x > 0).mean())\n",
    "# Use TotalPremium as a proxy for claim severity if ClaimAmount is not present\n",
    "def compute_claim_severity_fixed(df, group_col):\n",
    "    return df[df['TotalClaims'] > 0].groupby(group_col)['TotalPremium'].mean()\n",
    "province_sev = compute_claim_severity_fixed(df, 'Province')\n",
    "province_margin = compute_margin(df, 'Province')\n",
    "\n",
    "# Select two provinces for A/B testing\n",
    "prov_a, prov_b = 'Gauteng', 'Western Cape'\n",
    "group_a, group_b = split_groups(df, 'Province', prov_a, prov_b)\n",
    "\n",
    "# Robust chi-squared test for claim frequency\n",
    "def claim_frequency_contingency(group_a, group_b, metric):\n",
    "    a_no_claims = (group_a[metric] == 0).sum()\n",
    "    a_claims = (group_a[metric] > 0).sum()\n",
    "    b_no_claims = (group_b[metric] == 0).sum()\n",
    "    b_claims = (group_b[metric] > 0).sum()\n",
    "    return np.array([[a_no_claims, a_claims], [b_no_claims, b_claims]])\n",
    "\n",
    "contingency = claim_frequency_contingency(group_a, group_b, 'TotalClaims')\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "freq_test = {'test': 'chi2', 'statistic': chi2, 'p_value': p}\n",
    "\n",
    "sev_test = run_ttest(group_a[group_a['TotalClaims']>0], group_b[group_b['TotalClaims']>0], 'TotalPremium')\n",
    "margin_test = run_ttest(group_a, group_b, 'TotalPremium')\n",
    "\n",
    "print('Claim Frequency:', interpret_result(freq_test))\n",
    "print('Claim Severity:', interpret_result(sev_test))\n",
    "print('Margin:', interpret_result(margin_test))\n",
    "print(business_recommendation('provinces', freq_test, prov_a, prov_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b78b38",
   "metadata": {},
   "source": [
    "## Hypothesis 2: Risk Differences Between Zip Codes\n",
    "\n",
    "- **Null Hypothesis (H₀):** There are no risk differences between zip codes.\n",
    "- **Metrics:** Claim Frequency, Claim Severity, Margin\n",
    "- **Test:** Chi-squared (frequency), t-test (severity, margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f68665ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough unique postal codes for A/B testing.\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics for zip codes\n",
    "postal_counts = df['PostalCode'].value_counts()\n",
    "if len(postal_counts) < 2:\n",
    "    print(\"Not enough unique postal codes for A/B testing.\")\n",
    "else:\n",
    "    zip_a, zip_b = postal_counts.index[:2]\n",
    "    group_a, group_b = split_groups(df, 'PostalCode', zip_a, zip_b)\n",
    "    # Robust chi-squared test for claim frequency\n",
    "    contingency = claim_frequency_contingency(group_a, group_b, 'TotalClaims')\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "    freq_test = {'test': 'chi2', 'statistic': chi2, 'p_value': p}\n",
    "    # Use TotalPremium as a proxy for claim severity\n",
    "    sev_test = run_ttest(group_a[group_a['TotalClaims']>0], group_b[group_b['TotalClaims']>0], 'TotalPremium')\n",
    "    margin_test = run_ttest(group_a, group_b, 'TotalPremium')\n",
    "\n",
    "    print('Claim Frequency:', interpret_result(freq_test))\n",
    "    print('Claim Severity:', interpret_result(sev_test))\n",
    "    print('Margin:', interpret_result(margin_test))\n",
    "    print(business_recommendation('zip codes', freq_test, zip_a, zip_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b2d205",
   "metadata": {},
   "source": [
    "## Hypothesis 3: Margin Differences Between Zip Codes\n",
    "\n",
    "- **Null Hypothesis (H₀):** There are no significant margin (profit) differences between zip codes.\n",
    "- **Metric:** Margin\n",
    "- **Test:** t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e20f2a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough unique postal codes for margin difference test.\n"
     ]
    }
   ],
   "source": [
    "# Margin difference test for zip codes\n",
    "postal_counts = df['PostalCode'].value_counts()\n",
    "if len(postal_counts) < 2:\n",
    "    print(\"Not enough unique postal codes for margin difference test.\")\n",
    "else:\n",
    "    zip_a, zip_b = postal_counts.index[:2]\n",
    "    group_a, group_b = split_groups(df, 'PostalCode', zip_a, zip_b)\n",
    "    margin_test = run_ttest(group_a, group_b, 'TotalPremium')\n",
    "    print('Margin:', interpret_result(margin_test))\n",
    "    print(business_recommendation('zip code margin', margin_test, zip_a, zip_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268d0ff",
   "metadata": {},
   "source": [
    "## Hypothesis 4: Risk Differences Between Women and Men\n",
    "\n",
    "- **Null Hypothesis (H₀):** There are not significant risk differences between Women and Men.\n",
    "- **Metrics:** Claim Frequency, Claim Severity, Margin\n",
    "- **Test:** Chi-squared (frequency), t-test (severity, margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d153c991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Frequency: Fail to reject the null hypothesis (p = nan).\n",
      "Claim Severity: Fail to reject the null hypothesis (p = nan).\n",
      "Margin: Fail to reject the null hypothesis (p = nan).\n",
      "No significant difference found for gender between Female and Male. No change recommended.\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics for gender\n",
    "group_a, group_b = split_groups(df, 'Gender', 'Female', 'Male')\n",
    "# Robust chi-squared test for claim frequency\n",
    "def claim_frequency_contingency(group_a, group_b, metric):\n",
    "    a_no_claims = (group_a[metric] == 0).sum()\n",
    "    a_claims = (group_a[metric] > 0).sum()\n",
    "    b_no_claims = (group_b[metric] == 0).sum()\n",
    "    b_claims = (group_b[metric] > 0).sum()\n",
    "    return np.array([[a_no_claims, a_claims], [b_no_claims, b_claims]])\n",
    "contingency = claim_frequency_contingency(group_a, group_b, 'TotalClaims')\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "freq_test = {'test': 'chi2', 'statistic': chi2, 'p_value': p}\n",
    "# Use TotalPremium as a proxy for claim severity\n",
    "ev_test = run_ttest(group_a[group_a['TotalClaims']>0], group_b[group_b['TotalClaims']>0], 'TotalPremium')\n",
    "margin_test = run_ttest(group_a, group_b, 'TotalPremium')\n",
    "\n",
    "print('Claim Frequency:', interpret_result(freq_test))\n",
    "print('Claim Severity:', interpret_result(ev_test))\n",
    "print('Margin:', interpret_result(margin_test))\n",
    "print(business_recommendation('gender', freq_test, 'Female', 'Male'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
